
# ğŸ“ VIE Setup and Usage Guide

- [ğŸ“ VIE Setup and Usage Guide](#-vie-setup-and-usage-guide)
  - [ğŸ› ï¸ Setup Instructions](#ï¸-setup-instructions)
    - [ğŸ§‘â€ğŸ’» Run the Setup Script](#-run-the-setup-script)
  - [ğŸ“œ Requirements](#-requirements)
  - [âš™ï¸ Data preprocessing](#ï¸-data-preprocessing)
    - [1. Setup paths](#1-setup-paths)
  - [ğŸ”§ Tools](#-tools)
    - [2. ğŸ” Find object prompts using GDINO](#2--find-object-prompts-using-gdino)
    - [3. ğŸ¤– Generate object masks using GDINO + SAMv2](#3--generate-object-masks-using-gdino--samv2)
    - [4. âœ‹ Extracting Right/Left Hand BBoxes and 3D Meshes (HaMeR)](#4--extracting-rightleft-hand-bboxes-and-3d-meshes-hamer)
    - [5. Transfer Human Hand to Fetch Gripper](#5-transfer-human-hand-to-fetch-gripper)
    - [ğŸ§© To visualize scene, hamer hand pose \& transferred pose](#-to-visualize-scene-hamer-hand-pose--transferred-pose)
    - [ğŸ”§ Arguments](#-arguments)
    - [6. BundleSDF Docker Setup](#6-bundlesdf-docker-setup)
    - [7. Object Pose Estimation Using BundleSDF](#7-object-pose-estimation-using-bundlesdf)
  - [8. Run GSAM2 + BundleSDF for Real-World Object Pose Estimation](#8-run-gsam2--bundlesdf-for-real-world-object-pose-estimation)
    - [ğŸ‘‰ Object Pose Estimation with Multi-Frame Context:](#-object-pose-estimation-with-multi-frame-context)
  - [ğŸ—‚ï¸ Output Directory Structure After Data Processing](#ï¸-output-directory-structure-after-data-processing)
    - [ğŸ—‚ï¸ obj\_prompt\_mapper.json](#ï¸-obj_prompt_mapperjson)
  - [ğŸ™ Acknowledgments](#-acknowledgments)


## ğŸ› ï¸ Setup Instructions

To set up the environment and prepare the vie pipeline, run the following commands:

### ğŸ§‘â€ğŸ’» Run the Setup Script
```shell
# Remove all __pycache__ directories and .egg-info files recursively
find . -name "__pycache__" -type d -exec rm -rf {} + -o -name "*.egg-info" -type d -exec rm -rf {} +

# Make the setup script executable and run it
chmod +x ./setup_vie.sh
./setup_vie.sh
```

## ğŸ“œ Requirements

- Following modules are tested on **Python 3.10.15**
  - `robokit` (gdino+samv2). Example conda [env.yml](./conda-envs/gsam2-py3.10.yml).
  - `hamer` & `rfp-grasp-transfer`. Example conda [env.yml](./conda-envs/robokit-py3.10.yml).

- BundleSDF runs in [docker](BundleSDF/docker) with **Python 3.8**

https://github.com/user-attachments/assets/015088f9-7031-44b9-b1b4-f4ea75043109


## âš™ï¸ Data preprocessing

### 1. Setup paths
```shell
export PROJECT_ROOT=/path/to/hrt1
export VIE_ROOT=$PROJECT_ROOT/vie
export TASK_DATA_ROOT=/path/to/data/captured/task_x
```

## ğŸ”§ Tools

ğŸ“Œ Step Dependencies Overview:
- ğŸ” Step 2 â¡ï¸ Step 3:
    - Object prompt selection (GDINO) is required before generating masks.
- ğŸ” Step 3 â¡ï¸ Step 7 & 8:
    - Object masks from Step 3 are used in BundleSDF for pose estimation.
- ğŸ”“ Step 4:
    - Hand mesh extraction (HaMeR) can be performed independently.
- ğŸ” Step 3 â¡ï¸ Step 5:
    - Gripper transfer needs the hand mesh aligned with object masks.
- ğŸš€ Step 8:
    - Full real-world execution combining all outputs (masks, poses).
- ğŸ³ Docker Dependency (Steps 7 & 8)
  - These steps must be executed inside a Docker container.
  - Check Step 6 to start and enter docker.

<hr>

### 2. ğŸ” Find object prompts using GDINO
Use GDINO with a text prompt to identify the object of interest in the first frame:
```shell
cd $VIE_ROOT
python test_gdino_prompts.py \
    --input_dir $TASK_DATA_ROOT/rgb \
    --text_prompt <obj-text-prompt> \
    --infer_first_only

# Output will be saved at:
# $TASK_DATA_ROOT/out/gdino/<obj_text_prompt>
# Note: spaces in <obj-text-prompt> will be replaced with "_"
```
âœ… Once you've found a text prompt that successfully detects the object, use it in Step 3 to generate object masks across all frames.

<hr>


### 3. ğŸ¤– Generate object masks using GDINO + SAMv2
To use GDINO and SAMv2 for object bounding box detection and tracking in video frames:
```shell
cd $VIE_ROOT
python run_gdino_samv2.py --input_dir $TASK_DATA_ROOT/rgb --text_prompt <obj-text-prompt> --save_interval=1
# Output saved in:
# $TASK_DATA_ROOT/out/samv2/<obj_text_prompt>/obj_masks - object mask
# $TASK_DATA_ROOT/out/samv2/<obj_text_prompt>/masks_traj_overlayed - Trajectory + mask overlay + initial object bbox
```

<hr>

### 4. âœ‹ Extracting Right/Left Hand BBoxes and 3D Meshes (HaMeR)
![vie-hand](../media/data_capture/vie-hand.png)

This step extracts right(1) / left(0) hand bounding boxes and 3D hand meshes using HaMeR.

âœ… Assumptions:
- Only one person is present in the scene.
- Only frames containing at least one visible hand will be processed and saved under `out/hamer/model`.
```shell
cd $VIE_ROOT/hamer
python extract_hand_bboxes_and_meshes.py --opt_weight 100.0 --input_dir $TASK_DATA_ROOT/rgb
```

ğŸ“¤ Output Directory Structure:
- `$TASK_DATA_ROOT/out/hamer/extra_plots` â€“ Visualizations and debugging images
- `$TASK_DATA_ROOT/out/hamer/scene` â€“ RGB scene point cloud
- `$TASK_DATA_ROOT/out/hamer/model` â€“ HaMeR results including MANO parameters
- `$TASK_DATA_ROOT/out/hamer/3dhand` â€“ Aligned 3D hand meshes

ğŸ› ï¸ Known Issue (Python 3.10+)
If you encounter:
```shell
from collections import Mapping
ImportError: cannot import name 'Mapping' from 'collections'
```
âœ… Try this fix: `pip install --upgrade networkx`

<hr>

### 5. Transfer Human Hand to Fetch Gripper

This step requires the human hand mesh output from Step 4 (HaMeR). It maps the human hand configuration to the target robot gripper (e.g., Fetch gripper).

```shell
# Navigate to the repo and initialize submodules
cd $VIE_ROOT/rfp-grasp-transfer
git submodule update --init --recursive

# Run the hand-to-gripper transfer script
python transfer_from_hamer.py \
--mano_model_dir ../hamer/_DATA/data/mano/mano_v1_2/models/ \
--target_gripper fetch_gripper \
--debug_plots \
--input_dir $TASK_DATA_ROOT
```

ğŸ“¤ Output Directory Structure:
- `$TASK_DATA_ROOT/out/hamer/transfer_extra_plots` â€“ Visualizations and debugging plots
- `$TASK_DATA_ROOT/out/hamer/transfer_hand_mesh` â€“ Transfered 3D fetch gripper meshes

ğŸ› ï¸ Troubleshooting
If you see this error:
```
from collections import Mapping
ImportError: cannot import name 'Mapping' from 'collections'
```
âœ… Try this fix: `pip install --upgrade networkx`

---

### ğŸ§© To visualize scene, hamer hand pose & transferred pose
This script visualizes the combined 3D point clouds from:
- `hamer/scene/` â€” RGB scene point cloud
- `hamer/3dhand/` â€” Predicted human hand mesh
- `hamer/transfer_hand_mesh/` â€” Transferred gripper mesh

```shell
python ply_viewer_with_combined_ply.py \
--data_dir $DATA_ROOT \
--num_points 100000000 \
--auto_mode \
--fps 10
```

### ğŸ”§ Arguments

| Argument             | Type    | Default                    | Description                                                                 |
|----------------------|---------|----------------------------|-----------------------------------------------------------------------------|
| `--data_dir`         | `str`   | `./data/ply_sequence/`     | Base directory containing `scene`, `3dhand`, and `transfer_hand_mesh` dirs |
| `--num_points`       | `int`   | `10000000000`              | Max number of points to load from each PLY file                             |
| `--fps`              | `int`   | `5`                        | Frames per second in auto playback mode                                     |
| `--skip_viz_frames`  | `int`   | `1`                        | Show every Nth frame in auto/manual mode                                    |
| `--auto_mode`        | `flag`  | `False`                    | Auto-play the sequence in a loop                                            |
| `--left_hand`        | `flag`  | `False`                    | Load left-hand meshes (`*_0.ply`); right-hand (`*_1.ply`) by default        |


<hr>

### 6. BundleSDF Docker Setup 
BundleSDF runs in docker. First setup docker container and enter
```shell
cd $VIE_ROOT/BundleSDF/
./docker/start_docker.sh # start docker container
./docker/enter_docker.sh $PWD # enter docker container
```

### 7. Object Pose Estimation Using BundleSDF
Run object pose estimation on captured video frames using BundleSDF (in docker):
```shell
cd $VIE_ROOT/BundleSDF/
python run_pose_only_bsdf.py --mode run_video --video_dir $TASK_DATA_ROOT
```

ğŸ› ï¸ Troubleshooting
If you encounter the following error:
```shell
from ._ckdtree import cKDTree, cKDTreeNode
ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /opt/conda/envs/py38/lib/python3.8/site-packages/scipy/spatial/_ckdtree.cpython-38-x86_64-linux-gnu.so)
```
âœ… Try this fix: `pip install --upgrade scipy==1.10 yacs`

ğŸ’¡ Heuristic Tip:
- If the predicted pose lies entirely within the object mask, it can be considered valid.
- If not, the frame can be skippedâ€”this simple heuristic helps filter out incorrect poses efficiently.

<hr>


## 8. Run GSAM2 + BundleSDF for Real-World Object Pose Estimation
Use the following command to perform real-time object pose estimation (in docker) by combining source frames (human demo) with rollout frames (robot execution):
```shell
cd $VIE_ROOT
./run_bundlesdf.sh $TASK_DATA_ROOT <src-frames> <rollout-frames>
# Example:
# ./run_obj_pose_est.sh "./vie/_DATA/new-data-from-fetch-and-laptop/22tasks.latest/task_8_17s-use_hammer/" 15 5
```
ğŸ§© Arguments:
- `task-root-dir-path`: Task root dir path
- `src-frames`: Number of frames to extract from the human demonstration.
- `rollout-frames`: Number of frames to process during real-time rollout.

### ğŸ‘‰ Object Pose Estimation with Multi-Frame Context:
- ğŸ“¸ Uses multiple frames from both:
  - Source phase (human demonstration)
  - Rollout phase (real-time robot execution)
- ğŸ¯ Provides richer visual and temporal context for improved understanding
- ğŸš€ Significantly outperforms single-frame methods in Accuracy, Stability, Robustness
- âœ… Enables more reliable pose tracking across time during real-world execution

<hr>

## ğŸ—‚ï¸ Output Directory Structure After Data Processing
```
data_captured/
â”œâ”€â”€ task_1/
â”‚   â”œâ”€â”€ cam_K.txt                      # Camera intrinsics
â”‚   â”œâ”€â”€ rgb/                           # RGB frames
â”‚   â”‚   â”œâ”€â”€ 000000.jpg
â”‚   â”‚   â”œâ”€â”€ 000001.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ depth/                         # Aligned depth frames (in mm)
â”‚   â”‚   â”œâ”€â”€ 000000.png
â”‚   â”‚   â”œâ”€â”€ 000001.png
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ pose/                          # RT camera (npz)
â”‚   â”‚   â”œâ”€â”€ 000000.npz
â”‚   â”‚   â”œâ”€â”€ 000001.npz
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ out/                           # All derived outputs
â”‚       â”œâ”€â”€ gdino/
â”‚       â”‚   â””â”€â”€ <text-prompt>/         # GDINO raw detection outputs
â”‚       â”œâ”€â”€ samv2/
â”‚       â”‚   â”œâ”€â”€ <text-prompt>/             # Processed masks from GDINO + SAMv2
    â”‚       â”‚   â”œâ”€â”€ obj_masks/             # Binary masks per object
    â”‚       â”‚   â””â”€â”€ masks_traj_overlayed/  # RGB overlays with tracked masks
â”‚       â”œâ”€â”€ bundlesdf/
â”‚       â”‚   â”œâ”€â”€ demonstration/
â”‚       â”‚   â”‚   â””â”€â”€ obj_<1/2>/
â”‚       â”‚   â”‚       â”œâ”€â”€ ob_in_cam/             # Object point clouds
â”‚       â”‚   â”‚       â”œâ”€â”€ pose_overlayed_rgb/    # Pose visualizations
â”‚       â”‚   â”‚       â””â”€â”€ obj_prompt_mapper.json # Maps object index to prompt
â”‚       â”‚   â””â”€â”€ rollout/
â”‚       â”‚       â””â”€â”€ obj_<1/2>/                 # Same as demo but for execution
â”‚       â”‚           â”œâ”€â”€ ob_in_cam/
â”‚       â”‚           â”œâ”€â”€ pose_overlayed_rgb/
â”‚       â”‚           â””â”€â”€ obj_prompt_mapper.json
â”‚       â””â”€â”€ hamer/
â”‚           â”œâ”€â”€ extra_plots/          # Debug plots (optional)
â”‚           â”‚   â”œâ”€â”€ 000000.npz
â”‚           â”‚   â””â”€â”€ ...
â”‚           â”œâ”€â”€ scene/                # Full scene point clouds
â”‚           â”‚   â”œâ”€â”€ 000000.ply
â”‚           â”‚   â””â”€â”€ ...
â”‚           â”œâ”€â”€ model/                # MANO hand model outputs
â”‚           â”‚   â”œâ”€â”€ 000000.npz
â”‚           â”‚   â””â”€â”€ ...
â”‚           â””â”€â”€ 3dhand/               # 3D hand meshes aligned to scene
â”‚               â”œâ”€â”€ 000000.ply
â”‚               â””â”€â”€ ...
â”œâ”€â”€ task_2/
â””â”€â”€ task_.../
```


### ğŸ—‚ï¸ obj_prompt_mapper.json
- Maps object identifiers (e.g., obj_1, obj_2) to their corresponding text prompts used during SAMv2 mask generation.
- Ensures a consistent mapping between:
  - Source: `$TASK_DATA_ROOT/out/sam2/<text_prompt>/obj_masks`
  - Target: `$TASK_DATA_ROOT/masks/`
- This linkage is critical for enabling accurate object pose estimation during real-time execution.

ğŸ”— Example for single object:
```json
{
  "obj_1": "black_eraser"
}
```

ğŸ”— Example for dual objects:
```json
{
  "obj_1": "black_eraser",
  "obj_2": "whiteboard"
}
```


## ğŸ™ Acknowledgments

This project utilizes the following resources:
- [GDINO + SamV2](https://github.com/jishnujayakumar/robokit)
- [HaMeR](https://github.com/IRVLUTD/HaMeR)
- [rfp-grasp-transfer](https://github.com/IRVLUTD/rfp-grasp-transfer)
- [BundleSDF](https://github.com/jishnujayakumar/BundleSDF)
